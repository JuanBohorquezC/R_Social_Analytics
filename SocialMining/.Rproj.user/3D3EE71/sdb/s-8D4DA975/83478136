{
    "collab_server" : "",
    "contents" : "######################################################################\n######################## MONITOREO REDES SOCIALES ####################\n######################################################################\n\n# 1. CAMBIAR LAS RUTAS COMUNES\n# 2. CAMBIAR LA RUTA A FUNCIONES.R\n\n######### LIBRERIAS ###########\nlibrary(xml2)\nlibrary(rvest)\nlibrary(stringr)\nlibrary(plyr); library(dplyr)\nlibrary(twitteR)\nlibrary(ROAuth)\nlibrary(Rfacebook)\nlibrary(RCurl)\n\nsource('D:/Data/R/SocialMining/Funciones.r')\n\n######## RUTAS COMUNES #######\n\nposFile = 'D:/Data/R/SocialMining/Parametros/positive-words.txt'\nnegFile = 'D:/Data/R/SocialMining/Parametros/negative-words.txt'\nmunicipios = 'D:/Data/R/SocialMining/Parametros/Infraestructura.csv'\n\nsalidaWeb = 'D:/Data/R/SocialMining/WebCrawling/Salidas'\nsalidaTwitter = 'D:/Data/R/SocialMining/Twiter/salidas'\nsalidaFacebook = 'D:/Data/R/SocialMining/Facebook/Salidas'\nsalidaFinal = 'D:/Data/R/SocialMining/Salida'\n\n######## PARAMETROS COMUNES #######\nterm = 'arbelaez%ecopetrol'\n##term = 'Ecopetrol'\n\n####fbIDpage <- 159616034235\n#Ecopetrol: 117649051695\n#https://www.facebook.com/NoticiasAcacias/ 713234648746028\n#https://www.facebook.com/AlcaldiaAcaciasMeta 419761254869697\n\n##fbIDpage <- 419761254869697\n##ID pagina Ecopetrol: 117649051695\nfbIDpage <- 117649051695\n\n\n######################################################################\n########################     NOTICIAS GOOGLE      ####################\n######################################################################\n\nsetwd(salidaWeb)\n\nstrbusqueda1 <- \"https://www.google.com/search?hl=es&gl=co&tbm=nws&q=\" \nstrbusqueda1 <- paste(strbusqueda1, term, sep=\"\")\nstrbusqueda <- paste(strbusqueda1, \"&tbs=qdr:d,sbd:1\",sep=\"\")\n\nsearch_google <- read_html(strbusqueda)\nlinks = (html_attr(html_nodes(search_google, \"a\"), \"href\"))\na.texts = html_text(html_nodes(search_google, \"a\"))\n\ntitles = html_text(search_google %>% html_nodes(\"h3\") %>% html_node(\"a\"))\n\ndf.links = as.data.frame(links)\n\nindices = which(grepl(\"/url?\", as.character(links)) == TRUE)\n\ndf.links = as.data.frame(substr(df.links[indices,], 8, 1000))\n\ndivs = html_text(html_nodes(search_google, \"div\"))\ndivs.texts = texts = gsub(\"\\n||\\t||\\r\", \"\", divs)\ndf.divs.texts = as.data.frame(divs.texts)\n\ndivs.class = html_attr(html_nodes(search_google, \"div\"), \"class\")\ndf.divs.class = as.data.frame(divs.class)\n\ndf.divs = cbind(df.divs.texts, df.divs.class)\nnames(df.divs) = c(\"Text\", \"Clase\")\n\ndf.divs.indices = which(df.divs[,2] == \"st\")\n\ndf.divs.final = df.divs[df.divs.indices,]\n\nfuente = html_text(search_google %>% html_nodes(\"div\") %>% html_nodes(\"div\") %>% html_nodes(\"div\") %>% html_node(\"span\"))\nindices = which(nchar(fuente) > 0)\ndf.fuente = fuente[indices]\n\n################################\n\nfuente.time = NULL\n\nfor(i in 1:length(df.fuente)){\n  registro = tolower(df.fuente[i])\n  tiempo.fin = gregexpr(\" \", registro)[[1]]\n  tiempo.inicio = gregexpr(\"hace\", registro)[[1]]\n  \n  date.index = gregexpr(\"[0-9]\", substr(registro, tiempo.inicio, max(tiempo.fin) + 1))[[1]]\n  \n  date.marco = substr(registro, max(tiempo.fin) + 1, max(tiempo.fin) + 1)\n  \n  date.time = as.numeric(substring(substr(registro, tiempo.inicio, max(tiempo.fin)), date.index, date.index))\n  \n  date.time = as.numeric(substring(substr(registro, tiempo.inicio, max(tiempo.fin) + 1), min(date.index), max(date.index)))\n  \n  if(date.marco == \"h\"){\n    time = as.character(format(Sys.time() - date.time[1]*3600, '%Y-%m-%d %H:%M'))\n  } else\n  {\n    time = as.character(format(Sys.time() - date.time[1]*60, '%Y-%m-%d %H:%M'))\n  }\n  \n  fuente.time = append(fuente.time, time)\n  \n}\n################################\n\ndf.divs.final = cbind(df.divs.final, titles, df.fuente)\n\ndf.divs.final.1 = cbind(df.fuente, titles, as.character(df.divs.final[,1]), fuente.time)\ndf.divs.final.1 = as.data.frame(df.divs.final.1)\ndf.divs.final = df.divs.final.1\nnames(df.divs.final) = c(\"Fuente\", \"Titulo\", \"Resumen\", \"Fecha_Actualizacion\")\n\nrm(df.divs.final.1)\n\nwrite.table(df.divs.final, file='WebCrawling_original.txt', row.names=F,  col.names = T, sep = \"\\t\")\n\nif (file.exists('WebCrawling_stack.txt')==FALSE)\n  write.table(df.divs.final, file='WebCrawling_stack.txt', row.names=F,  col.names = T, sep = \"\\t\")\n\n#merge last access with cumulative file and remove duplicates\nstack <- read.table(file='WebCrawling_stack.txt', sep = \"\\t\", header = T, fill = T)\nstack <- rbind(stack, df.divs.final)\nstack <- subset(stack, !duplicated(stack$Titulo))\nwrite.table(stack, file='WebCrawling_stack.txt', row.names=F,  col.names = T, sep = \"\\t\")\n\n################ EVALUACION ######################\n\nscore.sentiment <- function(sentences, pos.words, neg.words, .progress='none')\n{\n  require(plyr)\n  require(stringr)\n  \n  \n  scores <- laply(sentences, function(sentence, pos.words, neg.words){\n    sentence <- gsub('[[:punct:]]', \"\", sentence)\n    sentence <- gsub('[[:cntrl:]]', \"\", sentence)\n    sentence <- tolower(sentence)\n    word.list <- str_split(sentence, ' ')\n    words <- unlist(word.list)\n    \n    pos.words <- tolower(as.character(pos.words[,1]))\n    neg.words <- tolower(as.character(neg.words[,1]))\n    \n    pos.matches <- match(words, pos.words)\n    neg.matches <- match(words, neg.words)\n    pos.matches <- !is.na(pos.matches)\n    neg.matches <- !is.na(neg.matches)\n    score <- sum(pos.matches) - sum(neg.matches)\n    return(score)\n  }, pos.words, neg.words, .progress=.progress)\n  scores.df <- data.frame(score=scores, text=sentences, date=stack$Fecha_Actualizacion, user=stack$Fuente)\n  return(scores.df)\n}\npos <- read.table(posFile, encoding = 'UTF-8') #folder with positive dictionary\nneg <- read.table(negFile, encoding = 'UTF-8') #folder with negative dictionary\n\nDataset <- stack\nDataset$Resumen <- as.factor(Dataset$Resumen)\nscores <- score.sentiment(paste(Dataset$Titulo, \" \", Dataset$Resumen), pos, neg, .progress='text')\nwrite.table(scores, file='WebCrawling_scores.txt', row.names=F, col.names = T, sep = '\\t') #save evaluation results into the file\n\ngeoloc = read.csv(file=municipios, header = TRUE, sep = \";\", dec = \",\")\ngeoloc = geoloc[, 3:6]\ngeoloc$MUNICIPIO =iconv(tolower(geoloc$MUNICIPIO), to=\"ASCII//TRANSLIT\")\n\ntexts = read.table(file='WebCrawling_stack.txt', sep = \"\\t\", header = T, fill = T)\ndates = texts$Fecha_Actualizacion\ntexts = texts$Resumen\n\ntexts = str_replace_all(texts,\"[^[:graph:]]\", \" \")\ntexts = str_replace_all(texts,\"[[:punct:]]\", \" \") \ntexts = str_replace_all(texts,\"[[:cntrl:]]\", \" \") \ntexts = tolower(texts)\nword.list = str_split(texts, ' ')\nwords = unlist(word.list)\n\nkeywords = NULL\n\nfor (i in 1:length(words)){\n  \n  if (nchar(words[i]) >= 4){\n    \n    keywords = append(keywords, words[i])\n    \n  }\n  \n}\n\nunicos = unique(keywords)\nunicos.formato = iconv(unicos, to=\"ASCII//TRANSLIT\")\nfrecuencia.list = NULL\n\nfor (i in 1:length(unicos)){\n  \n  frecuencia = length(which(unicos[i] == keywords))\n  frecuencia.list = append(frecuencia.list, frecuencia)\n  \n}\n\ngeo.yes = NULL\n\nfor (i in 1:length(unicos)){\n  \n  geo.len = length(which(unicos[i] == geoloc$MUNICIPIO))\n  \n  if (geo.len > 0){\n    \n    geo.bool = 1\n    x = geoloc$COORDENADA.X[which(geoloc$MUNICIPIO == unicos[i])[1]]\n    y = geoloc$COORDENADA.Y[which(geoloc$MUNICIPIO == unicos[i])[1]]\n    dane = geoloc$CODIGO.MUNICIPIO[which(geoloc$MUNICIPIO == unicos[i])[1]]\n    final.bool = cbind(geo.bool, x, y, dane)\n    geo.yes = rbind(geo.yes, final.bool)\n  }else{\n    \n    geo.bool = 0\n    x = 0\n    y = 0\n    dane = 0\n    final.bool = cbind(geo.bool, x, y, dane)\n    geo.yes = rbind(geo.yes, final.bool)\n    \n  } \n  \n}\n\nfrecuencias = cbind(unicos,frecuencia.list, geo.yes, format(Sys.time(), '%Y-%m-%d'))\nfrecuencias = as.data.frame(frecuencias)\nnames(frecuencias) = c('Palabra', 'Frecuencia', 'geo.bool', 'x', 'y', 'DANE', 'Actualizacion')\n\nmaximos = which(as.numeric(as.character(frecuencias$Frecuencia)) > 3)\n\ntendencias = frecuencias[maximos,]\n\n##############################################\n\nwords.date = NULL\n\nuntil.unicos = length(tendencias[,1])\nuntil.texts = length(texts)\n\nfor (i in 1:until.unicos){\n  palabra = as.character(tendencias[i,1])\n  for (j in 1:length(texts)){\n    count = gregexpr(palabra, texts[j])[[1]]\n    if(count[1] != -1){\n      contador = length(count)\n      words.date = rbind(words.date, cbind(id=j, palabra=palabra, count=contador, date=as.character(dates[j])))\n    } \n  }\n}\n\nwrite.table(words.date, file=paste('WebCrawling_words.txt', sep = ''), row.names=F,  col.names = T, sep = \"\\t\")\n\n##############################################\n\n\nwrite.table(tendencias[,1:2], file='WebCrawling_tendencia.txt', row.names=F,  col.names = T, sep = \"\\t\")\n\nfrecuenciastemp = frecuencias[which(as.numeric(as.character(frecuencias$geo.bool)) == 1),]\n\nwrite.table(frecuenciastemp, file='WebCrawling_Ecopetrol_stack_geo.txt', row.names=F,  col.names = T, sep = \"\\t\")\n\nscores = read.table(file = 'WebCrawling_scores.txt', sep = \"\\t\", header = T, fill = T)\n\ncolumnas.nuevas = c('Codigo', 'Municipio', 'X', 'Y', 'Calificacion')\nscores[,columnas.nuevas] = NA\n\nfor (i in 1:length(texts)){\n  \n  word.list.i = str_split(texts[i], ' ')\n  words.i = unlist(word.list.i)\n  \n  for (j in 1:length(words.i)){\n    \n    index.i = which(words.i[j] == frecuenciastemp$Palabra)\n    \n    if (length(index.i) > 0){\n      \n      scores$Municipio[i] =  words.i[j]\n      scores$X[i] = geoloc$COORDENADA.X[which(geoloc$MUNICIPIO == words.i[j])[1]]\n      scores$Y[i] = geoloc$COORDENADA.Y[which(geoloc$MUNICIPIO == words.i[j])[1]]\n      scores$Codigo[i] = geoloc$CODIGO.MUNICIPIO[which(geoloc$MUNICIPIO == words.i[j])[1]]\n    }\n  }\n  \n  score = scores$score[i]\n  if (score > 0){\n    \n    scores$Calificacion[i] = 'Positivo'\n    \n  } else if (score < 0){\n    \n    scores$Calificacion[i] = 'Negativo'\n    \n  } else {\n    \n    scores$Calificacion[i] = 'Neutral'\n    \n  }\n  \n}\n\nwrite.table(scores, file = 'WebCrawling_scores_geo.txt', row.names=F,  col.names = T, sep = \"\\t\")\n\n######################################################################\n########################    FIN NOTICIAS GOOGLE   ####################\n######################################################################\n\n######################################################################\n############################     TWITER      #########################\n######################################################################\n\nsetwd(salidaTwitter)\n\n#connect to API\n\nconsumerKey <- 'QN87DWoqgIGk9OvtjcfD8ntCm'\nconsumerSecret <- 'CJhZuJn5tJ9LFRFy9qjksYzazj27nnRZxuE6yb66IZSrNjuJSl'\naccessToken = '202032159-XzsHSnLW5efjHlAiWu7ZWp7GsBoh3D0voPgAnvdQ'\naccessSecret = 'jLqInr5UkrQPpSL8Y0ltGWjhP5mfIwG6mwn8ART85ilko'\n\nsetup_twitter_oauth(consumerKey, consumerSecret, accessToken, accessSecret)\n\nsearchTermTwitter(term, posFile = posFile, negFile = negFile)\n\ngeoloc = read.csv(file=municipios, header = TRUE, sep = \";\", dec = \",\")\ngeoloc = geoloc[, 3:6]\ngeoloc$MUNICIPIO =iconv(tolower(geoloc$MUNICIPIO), to=\"ASCII//TRANSLIT\")\n\ntexts = read.table(file=paste(term,'_stack.txt', sep = ''), sep = \"\\t\", header = T, fill = T)\ndates = texts$created\ntexts = texts$text\n\ntexts = str_replace_all(texts,\"[^[:graph:]]\", \" \")\ntexts = str_replace_all(texts,\"[[:punct:]]\", \" \") \ntexts = str_replace_all(texts,\"[[:cntrl:]]\", \" \") \ntexts = tolower(texts)\nword.list = str_split(texts, ' ')\nwords = unlist(word.list)\n\nkeywords = NULL\n\nfor (i in 1:length(words)){\n  \n  if (nchar(words[i]) >= 4){\n    \n    keywords = append(keywords, words[i])\n    \n  }\n  \n}\n\nunicos = unique(keywords)\nunicos.formato = iconv(unicos, to=\"ASCII//TRANSLIT\")\nfrecuencia.list = NULL\n\nfor (i in 1:length(unicos)){\n  \n  frecuencia = length(which(unicos[i] == keywords))\n  frecuencia.list = append(frecuencia.list, frecuencia)\n  \n}\n\ngeo.yes = NULL\n\nfor (i in 1:length(unicos)){\n  \n  geo.len = length(which(unicos[i] == geoloc$MUNICIPIO))\n  \n  if (geo.len > 0){\n    \n    geo.bool = 1\n    x = geoloc$COORDENADA.X[which(geoloc$MUNICIPIO == unicos[i])[1]]\n    y = geoloc$COORDENADA.Y[which(geoloc$MUNICIPIO == unicos[i])[1]]\n    dane = geoloc$CODIGO.MUNICIPIO[which(geoloc$MUNICIPIO == unicos[i])[1]]\n    final.bool = cbind(geo.bool, x, y, dane)\n    geo.yes = rbind(geo.yes, final.bool)\n  }else{\n    \n    geo.bool = 0\n    x = 0\n    y = 0\n    dane = 0\n    final.bool = cbind(geo.bool, x, y, dane)\n    geo.yes = rbind(geo.yes, final.bool)\n    \n  } \n  \n}\n\nfrecuencias = cbind(unicos,frecuencia.list, geo.yes, format(Sys.time(), '%Y-%m-%d %H:%M'))\nfrecuencias = as.data.frame(frecuencias)\nnames(frecuencias) = c('Palabra', 'Frecuencia', 'geo.bool', 'x', 'y', 'DANE', 'Actualizacion')\n\nmaximos = which(as.numeric(as.character(frecuencias$Frecuencia)) > 50)\n\ntendencias = frecuencias[maximos,]\n\n##############################################\n\nwords.date = NULL\n\nuntil.unicos = length(tendencias[,1])\nuntil.texts = length(texts)\n\nfor (i in 1:until.unicos){\n  palabra = as.character(tendencias[i,1])\n  for (j in 1:length(texts)){\n    count = gregexpr(palabra, texts[j])[[1]]\n    if(count[1] != -1){\n      contador = length(count)\n      words.date = rbind(words.date, cbind(id=j, palabra=palabra, count=contador, date=as.character(dates[j])))\n    } \n  }\n}\n\nwrite.table(words.date, file=paste('tw_',term,'_words.txt', sep = ''), row.names=F,  col.names = T, sep = \"\\t\")\n\n##############################################\n\n\nwrite.table(tendencias[,1:2], file=paste('tw_',term,'_tendencia.txt', sep = ''), row.names=F,  col.names = T, sep = \"\\t\")\n\nfrecuenciastemp = frecuencias[which(as.numeric(as.character(frecuencias$geo.bool)) == 1),]\n\nwrite.table(frecuenciastemp, file=paste('tw_',term,'_Ecopetrol_stack_geo.txt', sep = ''), row.names=F,  col.names = T, sep = \"\\t\")\n\nscores = read.table(file=paste(term,'_scores.txt', sep = ''), sep = \"\\t\", header = T, fill = T)\n\ncolumnas.nuevas = c('Codigo', 'Municipio', 'X', 'Y', 'Calificacion')\nscores[,columnas.nuevas] = NA\n\nfor (i in 1:length(texts)){\n  \n  word.list.i = str_split(texts[i], ' ')\n  words.i = unlist(word.list.i)\n  \n  for (j in 1:length(words.i)){\n    \n    index.i = which(words.i[j] == frecuenciastemp$Palabra)\n    \n    if (length(index.i) > 0){\n      \n      scores$Municipio[i] =  words.i[j]\n      scores$X[i] = geoloc$COORDENADA.X[which(geoloc$MUNICIPIO == words.i[j])[1]]\n      scores$Y[i] = geoloc$COORDENADA.Y[which(geoloc$MUNICIPIO == words.i[j])[1]]\n      scores$Codigo[i] = geoloc$CODIGO.MUNICIPIO[which(geoloc$MUNICIPIO == words.i[j])[1]]\n    }\n  }\n  \n  score = scores$score[i]\n  if (score > 0){\n    \n    scores$Calificacion[i] = 'Positivo'\n    \n  } else if (score < 0){\n    \n    scores$Calificacion[i] = 'Negativo'\n    \n  } else {\n    \n    scores$Calificacion[i] = 'Neutral'\n    \n  }\n  \n}\n\nwrite.table(scores, file=paste('tw_',term, '_scores_geo.txt', sep = ''), row.names=F,  col.names = T, sep = \"\\t\")\n\n######################################################################\n############################    FIN TWITTER   ########################\n######################################################################\n\n######################################################################\n###########################     FACEBOOK      ########################\n######################################################################\n\n#httr::set_config( config( ssl_verifypeer = 0L ) )\n#set_config(use_proxy(url='titan.ecopetrol.com.co',8080))\n\nsetwd(salidaFacebook)\n\n##app_id = \"839951899439143\"\napp_id = \"1974037539493552\"\n\n##app_secret = \"1ebfa719a5eb0b8ae8ea9ec37d5fcead\"\napp_secret = \"d57ba852a41d5a06df669a4a67f21722\"\n\n##fb_oauth <- fbOAuth(app_id, app_secret, extended_permissions = TRUE)\n\n##save(fb_oauth, file=\"fb_oauth\")\n\nload(\"fb_oauth\")\n\n#pages2 <- searchPages( string='Ecopetrol', token=fb_oauth)\npages <- searchPages( string=term, token=fb_oauth, 400)\n\n\n###Se lee la pagina de Ecopetrol sin comentarios\n###ID pagina Ecopetrol: 117649051695\n\nsearchInfacebook(fbIDpage, posFile, negFile)\n\ngeoloc = read.csv(file=municipios, header = TRUE, sep = \";\", dec = \",\")\ngeoloc = geoloc[, 3:6]\ngeoloc$MUNICIPIO =iconv(tolower(geoloc$MUNICIPIO), to=\"ASCII//TRANSLIT\")\n\ntexts = read.table(file='Ecopetrol_post_comments.txt', sep = \"\\t\", header = T, fill = F)\ndates = texts$created_time\ntexts = texts$message\n\ntexts = str_replace_all(texts,\"[^[:graph:]]\", \" \")\ntexts = str_replace_all(texts,\"[[:punct:]]\", \" \") \ntexts = str_replace_all(texts,\"[[:cntrl:]]\", \" \")\ntexts = str_replace_all(texts,\"\\r\", \" \")\ntexts = tolower(texts)\nword.list = str_split(texts, ' ')\nwords = unlist(word.list)\n\nkeywords = NULL\n\nfor (i in 1:length(words)){\n  \n  if (nchar(words[i]) >= 4){\n    \n    keywords = append(keywords, words[i])\n    \n  }\n  \n}\n\nunicos = unique(keywords)\nunicos.formato = iconv(unicos, to=\"ASCII//TRANSLIT\")\nfrecuencia.list = NULL\n\nfor (i in 1:length(unicos)){\n  \n  frecuencia = length(which(unicos[i] == keywords))\n  frecuencia.list = append(frecuencia.list, frecuencia)\n  \n}\n\ngeo.yes = NULL\n\nfor (i in 1:length(unicos)){\n  \n  geo.len = length(which(unicos[i] == geoloc$MUNICIPIO))\n  \n  if (geo.len > 0){\n    \n    geo.bool = 1\n    x = geoloc$COORDENADA.X[which(geoloc$MUNICIPIO == unicos[i])[1]]\n    y = geoloc$COORDENADA.Y[which(geoloc$MUNICIPIO == unicos[i])[1]]\n    dane = geoloc$CODIGO.MUNICIPIO[which(geoloc$MUNICIPIO == unicos[i])[1]]\n    final.bool = cbind(geo.bool, x, y, dane)\n    geo.yes = rbind(geo.yes, final.bool)\n  }else{\n    \n    geo.bool = 0\n    x = 0\n    y = 0\n    dane = 0\n    final.bool = cbind(geo.bool, x, y, dane)\n    geo.yes = rbind(geo.yes, final.bool)\n    \n  } \n  \n}\n\nfrecuencias = cbind(unicos,frecuencia.list, geo.yes, format(Sys.time(), '%Y-%m-%d %H'))\nfrecuencias = as.data.frame(frecuencias)\nnames(frecuencias) = c('Palabra', 'Frecuencia', 'geo.bool', 'x', 'y', 'DANE', 'Actualizacion')\n\nmaximos = which(as.numeric(as.character(frecuencias$Frecuencia)) > 1)\n\ntendencias = frecuencias[maximos,]\n\n##############################################\n\nwords.date = NULL\n\nuntil.unicos = length(tendencias[,1])\nuntil.texts = length(texts)\n\nfor (i in 1:until.unicos){\n  palabra = as.character(tendencias[i,1])\n  for (j in 1:length(texts)){\n    count = gregexpr(palabra, texts[j])[[1]]\n    if(count[1] != -1){\n      contador = length(count)\n      words.date = rbind(words.date, cbind(id=j, palabra=palabra, count=contador, date=as.character(dates[j])))\n    } \n  }\n}\n\nwrite.table(words.date, file=paste('fb_words.txt', sep = ''), row.names=F,  col.names = T, sep = \"\\t\")\n\n##############################################\n\n\nwrite.table(tendencias[,1:2], file='fb_tendencia.txt', row.names=F,  col.names = T, sep = \"\\t\")\n\nfrecuenciastemp = frecuencias[which(as.numeric(as.character(frecuencias$geo.bool)) == 1),]\n\nwrite.table(frecuenciastemp, file='Ecopetrol_stack_geo.txt', row.names=F,  col.names = T, sep = \"\\t\")\n\nscores = read.table(file='Ecopetrol_score.txt', sep = \"\\t\", header = T)\n\ncolumnas.nuevas = c('Codigo', 'Municipio', 'X', 'Y', 'Calificacion')\nscores[,columnas.nuevas] = NA\n\nfor (i in 1:length(texts)){\n  \n  word.list.i = str_split(texts[i], ' ')\n  words.i = unlist(word.list.i)\n  \n  for (j in 1:length(words.i)){\n    \n    index.i = which(words.i[j] == frecuenciastemp$Palabra)\n    \n    if (length(index.i) > 0){\n      \n      scores$Municipio[i] =  words.i[j]\n      scores$X[i] = geoloc$COORDENADA.X[which(geoloc$MUNICIPIO == words.i[j])[1]]\n      scores$Y[i] = geoloc$COORDENADA.Y[which(geoloc$MUNICIPIO == words.i[j])[1]]\n      scores$Codigo[i] = geoloc$CODIGO.MUNICIPIO[which(geoloc$MUNICIPIO == words.i[j])[1]]\n    }\n  }\n  \n  score = scores$score[i]\n  if (score > 0){\n    \n    scores$Calificacion[i] = 'Positivo'\n    \n  } else if (score < 0){\n    \n    scores$Calificacion[i] = 'Negativo'\n    \n  } else {\n    \n    scores$Calificacion[i] = 'Neutral'\n    \n  }\n  \n}\n\nwrite.table(scores, file='fb_Ecopetrol_scores_geo.txt', row.names=F,  col.names = T, sep = \"\\t\")\n\n######################################################################\n###########################    FIN FACEBOOK   ########################\n######################################################################\n\n######################################################################\n####################   GENERACION ARCHIVOS FINALES   #################\n######################################################################\n\n########### CAMBIAR RUTA ##############\nsetwd(salidaTwitter)\n\nscores = read.table(file=paste('tw_',term, '_scores_geo.txt', sep = ''), sep = \"\\t\", header = T, fill = T)\ntendencia = read.table(file=paste('tw_',term, '_tendencia.txt', sep = ''), sep = \"\\t\", header = T, fill = T)\npalabras = read.table(file=paste('tw_',term,'_words.txt', sep = ''), sep = \"\\t\", header = T, fill = T)\n\nscores[,'Fuente'] = NA\nscores$Fuente = 'Twitter'\n\ntendencia[,'Fuente'] = NA\ntendencia$Fuente = 'Twitter'\n\npalabras[,'Fuente'] = NA\npalabras$Fuente = 'Twitter'\n\n\nsetwd(salidaFacebook)\n\nscoresFb = read.table(file='fb_Ecopetrol_scores_geo.txt', sep = \"\\t\", header = T, fill = T)\ntendenciaFb = read.table(file='fb_tendencia.txt', sep = \"\\t\", header = T, fill = T)\npalabrasFb = read.table(file='fb_words.txt', sep = \"\\t\", header = T, fill = T)\n\nscoresFb[,'Fuente'] = NA\nscoresFb$Fuente = 'Facebook'\n\ntendenciaFb[,'Fuente'] = NA\ntendenciaFb$Fuente = 'Facebook'\n\npalabrasFb[,'Fuente'] = NA\npalabrasFb$Fuente = 'Facebook'\n\n\nsetwd(salidaWeb)\n\nscoresWC = read.table(file='WebCrawling_scores_geo.txt', sep = \"\\t\", header = T, fill = T)\ntendenciaWC = read.table(file='WebCrawling_tendencia.txt', sep = \"\\t\", header = T, fill = T)\npalabrasWC = read.table(file='WebCrawling_words.txt', sep = \"\\t\", header = T, fill = T)\n\nscoresWC[,'Fuente'] = NA\nscoresWC$Fuente = 'WebCrawling'\n\ntendenciaWC[,'Fuente'] = NA\ntendenciaWC$Fuente = 'WebCrawling'\n\npalabrasWC[,'Fuente'] = NA\npalabrasWC$Fuente = 'WebCrawling'\n\nscoresTotal = rbind(scores, scoresFb, scoresWC)\ntendenciaTotal = rbind(tendencia, tendenciaFb, tendenciaWC)\npalabrasTotal = rbind(palabras, palabrasFb, palabrasWC)\n\nscoresTotal[,'Total'] = NA\nscoresTotal$Total = 1\n\ntendenciaTotal[,'Total'] = NA\ntendenciaTotal$Total = 1\n\npalabrasTotal[,'Total'] = NA\npalabrasTotal$Total = 1\n\nsetwd(salidaFinal)\nwrite.table(scoresTotal, file='scores_geo_total.txt', row.names=F,  col.names = T, sep = \"\\t\")\nwrite.table(tendenciaTotal, file='tendencia_total.txt', row.names=F,  col.names = T, sep = \"\\t\")\nwrite.table(palabrasTotal, file='palabras_total.txt', row.names=F,  col.names = T, sep = \"\\t\")\n\n",
    "created" : 1502224125573.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "3773557977",
    "id" : "83478136",
    "lastKnownWriteTime" : 1502856066,
    "last_content_update" : 1502856069751,
    "path" : "D:/Data/R/SocialMining/SocialMedia.r",
    "project_path" : "SocialMedia.r",
    "properties" : {
    },
    "relative_order" : 3,
    "source_on_save" : false,
    "source_window" : "",
    "type" : "r_source"
}